
关于 Prompt 工程的一些心得

关于prompt engineering，这个话题差不多有两年了，这个词虽然已经过气，尤其最近被Karpathy的新词Context Engineering盖过，不过还是想强调一下prompt的重要性，prompt的质量直接关乎到AI交付结果的质量。然而实际很多开发同学，对prompt技巧掌握并不多，那让AI产出好代码的上限可能就不高了。

我们平时工作中拒绝接受一句话需求，同样，我们和AI沟通时候，应该尽量避免一句话需求，而是尝试将需求描述清楚。这和我们平时工作时候和同事沟通的情况也是一样的，如果表达不清楚，对方也是不知道要做什么，那交付的产物也是五花八门的。

在开始使用 AI Coding 之前，是有必要系统学习一下Prompt 技巧，对后续使用效果影响是很大。

我的两个经验：

1. 清晰的需求描述：

如果一个需求不能描述出来，那么谨慎将任务交给AI，因为你可能获取到的是惊喜，也可能是失望。举个例子，作为服务端同学，如果没法用语言描述前端这个输入框的视觉效果，那就没办法让AI实现前端代码

另外，在中文表达的时候可能存在二义性，可以中英文混合描述来表达需求。

前后端接口对齐

根据服务端接口/v1/api/chat 的request schema 和 response schema，调整一下前端接口

2.使用结构化的方式表示Prompt
比如说使用COSTAR框架，他是2023年新加坡prompt大赛冠军总结出来的一个提示词编写框架，他将Prompt分成了Context、Objective、Style、Tone、Audience、Response这几个部分，分别表示任务的背景、agent的目标、风格、回复预期、受众以及响应格式要求。我经常会将style、tone、audience做一些修改，加入一些对agent的要求。
另外，使用claude模型的时候，可以使用伪xml的结构做结构化，claude模型对于伪xml的理解更好。比如：

<<这是你的角色>>{your_role}<</这是你的角色>>

<<你的任务>>{your_task}<</你的任务>>

<<要求>>{specification}<</要求>>

<<输出格式>><</输出格式>>...

3.让AI协助将需求明确清楚，然后再做prompt engineering

在高效写prompt，或者明确需求这块，可以借助一些AI的工具，提升写prompt的效率。比如openai的prompt工具，也可以自己写一个prompt优化的agent。Claude在写prompt template这方面的效果比较不错


合理划分AI任务边界

在尝试修改生产级别的代码时，我一般会根据任务复杂度和自身能力范围合理分配 AI 的工作，按照我自己的能力范围划分为3个类别：

1. 能力范围内的任务：实现逻辑是清晰的，实现需要花很多时间让 AI 处理逻辑清晰但实现耗时的任务，可以显著提升效率。我把这类任务称为"搬砖提效"，常见的如CRUD，稍微复杂一点的像需求文档是非常清晰的，技术设计完善，性能、稳定性等方案也已经完善，剩下就是coding实现。

2. 略超出能力范围的任务：如果我通过调研、短期学习，就可以解决的，那我也会把这部分任务交给AI去解决。，比如我在一个项目环境里面需要调用阿里云 SDK ，他并没有提供javascript版本的签名，我需要详细文档阅读、参考python源码，改成js的版本。这种任务交给AI实现会非常方便，一方面他有能力去fetch官方的文档阅读，另外，对于一些流行的模型，比如Claude，他已经把主流的官方文档都已经训练过了，甚至不用阅读，就可以凭借内生的知识就可以帮我们补全。

3. 远超能力范围的任务：对于自己完全不熟悉的技术领域，不建议完全依赖 AI，除非这个代码仅仅只是用于demo用途。有个翻车例子是，我对React Native了解甚少，有个非常紧急的项目，期望用Claude Code生成一个React Native项目。AI前期代码写的很快，基本上半天就有一个可以跑在手机上的demo出来了。但是到了项目后期，想要加更多效果，就显得非常困难了。代码量越来越多，冗余代码问题、设计问题都藏在底下不得而知，效率变低，成本变高。最后还是回到使用熟悉的语言。

题外话: 越发觉得全栈技能，对于现在鞭笞AI干活显得非常重要。


小步快跑，每一步需要可验证

不要等代码全生成了，然后一次性调试，好的代码应该像细菌🦠一样（by Karpathy），精炼，模块化，闭包( copy paste-able)。再举一个翻车例子，当时按照需求/技术文件让AI进行生成全部代码，然后调试，结果AI告诉我这不是一个react-native的项目，直接崩溃。当时没舍得从头来过，进入无休止的鞭笞、调试、PUA，成本很高，最后不得已，还是重头开发...


AI生成的方案和代码必须要Review

除非需求极其清晰，否则不要期望一次命令就能完成一个完整需求，AI认为的完成，有可能并不是实际的完成。一方面可能会因为上下文长度的原因，遗忘，或者产生幻觉。 另外一方面对于项目的了解程度的片面性，生产出来的代码质量或技术方案不够好。

因此，我对AI生产的态度其实有转变，从部分信任到不信任，类似于防御性编程，在系统代码行增加到2万行左右时，对他生产的方案或者代码，我会详细的Review，确保代码投入到生产是没有问题的。

防御性编程的好处是提高代码质量和可靠性，但也需要平衡，过度会导致效率低下。中间过程中，通常也需要多轮沟通，减少信息传递过程中存在理解差异。其次让 AI 编写单元测试 ，利用 AI 为生成的代码编写单元测试，这是一种验证代码质量的有效方法，需要注意的是，不要盲目信任单元测试，AI 也可能为了让测试通过而采取一些技巧，因此仍需人工审核测试质量。

有两个例子，一个是AI为了能通过单元测试，修改了技术方案，实际仅仅是安装包依赖问题。另外一个例子是，多轮修复bug不成功后，AI偷懒修改了测试代码，做了mock数据让单元测试通过了。

频繁提交到git仓库

AI 通常能给出详细的 git commit 信息，充分利用这一点。AI非常熟悉git指令，能了解代码仓库过去都修改了哪些内容。因此git history就是项目的另外一份README.md，抑或者是上下文。

另外，频繁提交有助于在问题出现时方便回滚。

有效管理上下文

最近 Karpathy 提出了"上下文工程"(Context Engineering)"的概念。虽然现代模型支持 128K 甚至 192K 的上下文长度，但在编码场景下，这些上下文往往仍然不足，因为模型需要阅读和理解大量code文件，一下子就把上下文塞得差不多了。

像Claude Code这类的工具，在上下文做了很多优化 但是上下文越长，AI 生成代码出现幻觉的概率就越高，后续修正过程会消耗更多资源。
因此，我会人工辅助管理上下文：

1. 提供精确信息
当已确定修改范围时，应提供准确的文件路径和相关细节。

先通过与 AI 逐步沟通，获取并明确关键信息，形成清晰上下文后，再让AI执行。

2. 信息压缩策略
手动筛选重要信息，只保留有价值的部分。举个例子，我们在让AI修复一些执行错误的时候，如果把全部Exception信息丢给AI，比如Java抛出来的Exception，会非常长。想象一下我们自己去解决问题的时候，往往也是定位几行有用信息。

图片

3. 控制任务粒度
执行复杂任务需要较高的 prompt 技巧和使用经验，且难以验证细节。尝试过让AI写2个半小时的代码，一直在运行。但是对于结果，其实我们要花很多时间去做验证，review的成本会非常高。

过于复杂的任务可能超出上下文长度限制，导致 AI 遗忘早期任务内容。之前尝试让AI生成上百个单元测试，执行过程有过半没成功。尝试在依据命令里面让AI逐一修复，AI执行到最后会告知执行完成，实际只修复了几个。原因就是上下文太长了，他不记得任务列表。

4. 利用外部记忆
针对上面这种情况，我的做法是，将失败的任务手动编辑出来，并存储在一个外部文档中，然后告诉AI去逐个修复。

test_result.md里面记录了运行单元测试失败的case以及异常的信息，请从上往下进行修复。

对于每一个test case，代码修复完成后，通过运行pytest检查case是否执行成功。
若test case运行成功，在test_result.md里面标记完成。

任务结束前，请检查test_result.md文档，确保失败的测试用例全部修复。
也可以尝试使用 mem0 等 MCP 工具辅助管理上下文，我还没来得及尝试，有试过的同学可以分享一下。

5. 知识库很重要
对于已有项目，如果希望长期让AI持续进来改动，请务必先给他提供更多的信息，以及一个良好的信息获取方式。

像Claude Code，提供了 /init 指令，目的是为了让AI快速了解项目的背景、技术架构等，知识库记录了业务需求、技术规约、常见的工程流程等信息。

对于一个已经存在的工程项目，我强烈建议先让AI针对代码写说明文档(README.md)，然后再让他参与到写代码 。